{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/aaronbroderick/Desktop/Data Science Folders/Data/FineFoods.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = list(df.ProductId.value_counts().index)\n",
    "title = w[50]\n",
    "product = df[df['ProductId'] == title]\n",
    "product = product.reset_index(drop = True)\n",
    "product = pd.DataFrame(product)\n",
    "product.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gap statistic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "def calc_inertia(ag, data):\n",
    "    labels = ag.labels_\n",
    "    \n",
    "    W = 0\n",
    "    for l in labels:\n",
    "        Dl = 1-cosine_similarity(data[labels==l, :])\n",
    "        Sd = 0.5 * np.sum(Dl**2) / Dl.shape[0]\n",
    "        W = W + Sd\n",
    "    return W\n",
    "        \n",
    "\n",
    "#This function always uses cosine distance, probably should augment this to take an arbitrary distance function\n",
    "def optimalK(data, nrefs=3, maxClusters=15):\n",
    "    \"\"\"\n",
    "    Calculates KMeans optimal K using Gap Statistic from Tibshirani, Walther, Hastie\n",
    "    Params:\n",
    "        data: ndarry of shape (n_samples, n_features), assumed to be tf-idf with unit-normalized rows\n",
    "        nrefs: number of sample reference datasets to create\n",
    "        maxClusters: Maximum number of clusters to test for\n",
    "    Returns: (gaps, optimalK)\n",
    "    \"\"\"\n",
    "    N = data.shape[0]\n",
    "    maxClusters = min(maxClusters, max(2, N/3))\n",
    "    gaps = np.zeros((len(range(1, maxClusters)),))\n",
    "    resultsdf = pd.DataFrame({'clusterCount':[], 'gap':[]})\n",
    "    for gap_index, k in enumerate(range(1, maxClusters)):\n",
    "\n",
    "        # Holder for reference dispersion results\n",
    "        refDisps = np.zeros(nrefs)\n",
    "\n",
    "        # For n references, generate random sample and perform kmeans getting resulting dispersion of each loop\n",
    "        for i in range(nrefs):\n",
    "            \n",
    "            # Create new random reference set\n",
    "            randomReference = np.zeros(data.shape)\n",
    "            for qq in range(0, data.shape[1]):\n",
    "                randomReference[:, qq] = np.max(data[:, qq]) * np.random.sample(size=(data.shape[0],))\n",
    "            norms = np.sqrt(np.sum(randomReference**2, axis=1))\n",
    "            norms.shape = (len(norms), 1)\n",
    "            norms = np.tile(norms, (1, data.shape[1]))\n",
    "            randomReference = randomReference / norms #Normalize for consistency\n",
    "            \n",
    "            # Fit to it\n",
    "            cos_dist = 1-cosine_similarity(randomReference)\n",
    "            ag = AgglomerativeClustering(n_clusters=k)\n",
    "            ag.fit(cos_dist)\n",
    "            \n",
    "            #Calculate the deviation\n",
    "            refDisp = calc_inertia(ag, randomReference)\n",
    "            refDisps[i] = refDisp\n",
    "\n",
    "        # Fit cluster to original data and create dispersion\n",
    "        cos_dist = 1-cosine_similarity(data)\n",
    "        ag = AgglomerativeClustering(n_clusters=k)\n",
    "        ag.fit(cos_dist)\n",
    "        \n",
    "        origDisp = calc_inertia(ag, cos_dist)\n",
    "\n",
    "        # Calculate gap statistic\n",
    "        gap = np.log(np.mean(refDisps)) - np.log(origDisp)\n",
    "\n",
    "        # Assign this loop's gap statistic to gaps\n",
    "        gaps[gap_index] = gap\n",
    "        \n",
    "        resultsdf = resultsdf.append({'clusterCount':k, 'gap':gap}, ignore_index=True)\n",
    "\n",
    "    return (gaps.argmax() + 1, resultsdf)  # Plus 1 because index of 0 means 1 cluster is optimal, index 2 = 3 clusters are optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Delete stop words, remove all words except nouns.\n",
    "## Delete some annoying hypertext\n",
    "\n",
    "parser = spacy.load('en')\n",
    "def parse_text(string):\n",
    "    z = parser(string)\n",
    "    important_words = [word for word in z if word.is_stop==False and  word.pos_=='NOUN']\n",
    "    important_words = [str(word) for word in important_words]\n",
    "    important_words = ' '.join(important_words)\n",
    "    #important_words = important_words.replace('<br','')  \n",
    "    #important_words = important_words.replace('br','')\n",
    "    return important_words\n",
    "\n",
    "product['parse'] = product.Text.apply(parse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Remove overly common nouns\n",
    "common_words = ['coffee','cups', 'cup']\n",
    "\n",
    "def clean(string):\n",
    "    for i in range(len(common_words)):\n",
    "        string = string.replace(common_words[i],'')\n",
    "    return string\n",
    "\n",
    "product['parse'] = product.parse.apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the number of clusters to use using the gap statistic.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "product_tfidf = vectorizer.fit_transform(product['parse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "   clusterCount       gap\n",
      "0           1.0  7.142907\n",
      "1           2.0  6.758828\n",
      "2           3.0  6.773918\n",
      "3           4.0  7.275388\n",
      "4           5.0  7.233081\n",
      "5           6.0  7.094605\n",
      "6           7.0  6.942918\n",
      "7           8.0  7.147632\n",
      "8           9.0  6.993363\n"
     ]
    }
   ],
   "source": [
    "##  Calculate the gap statistic to determine number of clusters\n",
    "g, df = optimalK(product_tfidf, maxClusters = 10)\n",
    "print (g)\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create word lists for 4 clusters using LSA, LDA and NNMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the word list.\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# Number of topics.\n",
    "ntopics=4\n",
    "\n",
    "# Linking words to topics\n",
    "def word_topic(tfidf,solution, wordlist):\n",
    "    \n",
    "    # Loading scores for each word on each topic/component.\n",
    "    words_by_topic=tfidf.T * solution\n",
    "    # Linking the loadings to the words in an easy-to-read way.\n",
    "    components=pd.DataFrame(words_by_topic,index=wordlist)\n",
    "    \n",
    "    return components\n",
    "\n",
    "# Extracts the top N words and their loadings for each topic.\n",
    "def top_words(components, n_top_words):\n",
    "    n_topics = range(components.shape[1])\n",
    "    index= np.repeat(n_topics, n_top_words, axis=0)\n",
    "    topwords=pd.Series(index=index)\n",
    "    for column in range(components.shape[1]):\n",
    "        # Sort the column so that highest loadings are at the top.\n",
    "        sortedwords=components.iloc[:,column].sort_values(ascending=False)\n",
    "        # Choose the N highest loadings.\n",
    "        chosen=sortedwords[:n_top_words]\n",
    "        # Combine loading and index into a string.\n",
    "        chosenlist=chosen.index +\" \"+round(chosen,2).map(str) \n",
    "        topwords.loc[column]=chosenlist\n",
    "    return(topwords)\n",
    "\n",
    "# Number of words to look at for each topic.\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSA\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "svd= TruncatedSVD(ntopics)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "product_lsa = lsa.fit_transform(product_tfidf)\n",
    "\n",
    "components_lsa = word_topic(product_tfidf, product_lsa, terms)\n",
    "\n",
    "topwords=pd.DataFrame()\n",
    "topwords['LSA']=top_words(components_lsa, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronbroderick/anaconda/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "lda = LDA(n_components=ntopics, \n",
    "          doc_topic_prior=None, # Prior = 1/n_documents\n",
    "          topic_word_prior=1/ntopics,\n",
    "          learning_decay=0.7, # Convergence rate.\n",
    "          learning_offset=10.0, # Causes earlier iterations to have less influence on the learning\n",
    "          max_iter=10, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          evaluate_every=-1, # Do not evaluate perplexity, as it slows training time.\n",
    "          mean_change_tol=0.001, # Stop updating the document topic distribution in the E-step when mean change is < tol\n",
    "          max_doc_update_iter=100, # When to stop updating the document topic distribution in the E-step even if tol is not reached\n",
    "          n_jobs=-1, # Use all available CPUs to speed up processing time.\n",
    "          verbose=0, # amount of output to give while iterating\n",
    "          random_state=0\n",
    "         )\n",
    "\n",
    "product_lda = lda.fit_transform(product_tfidf) \n",
    "\n",
    "components_lda = word_topic(product_tfidf, product_lda, terms)\n",
    "topwords['LDA']=top_words(components_lda, n_top_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NNMF\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(alpha=0.0, \n",
    "          init='nndsvdar', # how starting value are calculated\n",
    "          l1_ratio=0.0, # Sets whether regularization is L2 (0), L1 (1), or a combination (values between 0 and 1)\n",
    "          max_iter=200, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          n_components=ntopics, \n",
    "          random_state=0, \n",
    "          solver='cd', # Use Coordinate Descent to solve\n",
    "          tol=0.0001, # model will stop if tfidf-WH <= tol\n",
    "          verbose=0 # amount of output to give while iterating\n",
    "         )\n",
    "product_nmf = nmf.fit_transform(product_tfidf) \n",
    "\n",
    "components_nmf = word_topic(product_tfidf, product_nmf, terms)\n",
    "\n",
    "topwords['NNMF']=top_words(components_nmf, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "             LSA           LDA            NNMF\n",
      "0   flavor 33.21   taste 11.25      flavor 6.4\n",
      "0    taste 16.12  flavor 10.65       blend 1.0\n",
      "0  morning 10.92  morning 5.38       time 0.92\n",
      "0  coconut 10.33    aroma 5.33    morning 0.89\n",
      "0     blend 9.07  vanilla 4.87    caramel 0.88\n",
      "0      time 8.64  husband 4.69    coconut 0.85\n",
      "0        br 8.05  caramel 4.42      aroma 0.84\n",
      "0      roast 7.9    price 4.36      taste 0.79\n",
      "0   product 7.89  brewing 3.78  favorites 0.79\n",
      "0   vanilla 7.67   product 3.7    vanilla 0.77\n",
      "Topic 1:\n",
      "             LSA           LDA          NNMF\n",
      "1    taste 18.46   flavor 7.96    taste 6.73\n",
      "1        br 3.46  product 3.46   flavor 1.13\n",
      "1   tasting 3.28    taste 3.34       br 0.98\n",
      "1       day 2.88      lot 3.08  coconut 0.95\n",
      "1   brewing 2.45  reviews 2.92  morning 0.76\n",
      "1     decaf 2.14      try 2.68  brewing 0.72\n",
      "1       way 2.09  morning 2.05  husband 0.61\n",
      "1   coconut 2.08     wife 1.99    aroma 0.58\n",
      "1   husband 2.02    blend 1.94  product 0.57\n",
      "1  purchase 1.97     time 1.91  tasting 0.52\n",
      "Topic 2:\n",
      "               LSA           LDA            NNMF\n",
      "2      roast 10.65   flavor 4.94      roast 4.47\n",
      "2     product 7.34  product 4.55     flavor 1.03\n",
      "2       decaf 5.24    taste 3.68      blend 1.03\n",
      "2       blend 4.76  morning 2.63    flavors 0.81\n",
      "2          br 3.77  creamer 2.41         br 0.61\n",
      "2  bitterness 3.29    price 2.17  favorites 0.53\n",
      "2        deal 3.12    money 2.01        deal 0.5\n",
      "2     tasting 2.88    roast 1.99      taste 0.47\n",
      "2     machine 2.78    brand 1.83      decaf 0.47\n",
      "2       price 2.46  coconut 1.82       time 0.46\n",
      "Topic 3:\n",
      "                LSA             LDA              NNMF\n",
      "3      product 9.78    flavor 17.34       product 4.8\n",
      "3        price 4.88     roast 10.51        flavor 1.3\n",
      "3      vanilla 4.33      taste 9.59           br 1.18\n",
      "3        decaf 2.32      blend 6.17        blend 1.01\n",
      "3  description 2.23    flavors 6.01         price 1.0\n",
      "3        money 2.06    coconut 5.95        taste 0.89\n",
      "3          box 2.02       time 5.68         decaf 0.8\n",
      "3        house 1.44  favorites 5.44  description 0.67\n",
      "3      creamer 1.44         br 5.24      vanilla 0.66\n",
      "3       choice 1.44      decaf 5.17          fan 0.64\n"
     ]
    }
   ],
   "source": [
    "for topic in range(ntopics):\n",
    "    print('Topic {}:'.format(topic))\n",
    "    print(topwords.loc[topic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a .csv to send to Plotly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topwords = topwords.reset_index(drop = True)\n",
    "topic1 = topwords[:10]\n",
    "topic2 = topwords[10:20]\n",
    "topic3 = topwords[20:30]\n",
    "topic4 = topwords[30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flavor 33.23'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = topic1.LSA[0]\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSA_topics(string):\n",
    "    x = string.split()\n",
    "    return x[0],float(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>product</td>\n",
       "      <td>9.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>price</td>\n",
       "      <td>4.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>description</td>\n",
       "      <td>2.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>box</td>\n",
       "      <td>2.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>money</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>2.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>decaf</td>\n",
       "      <td>1.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>br</td>\n",
       "      <td>1.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lot</td>\n",
       "      <td>1.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pods</td>\n",
       "      <td>1.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words  score\n",
       "0      product   9.95\n",
       "1        price   4.37\n",
       "2  description   2.44\n",
       "3          box   2.19\n",
       "4        money   2.08\n",
       "5      vanilla   2.05\n",
       "6        decaf   1.88\n",
       "7           br   1.52\n",
       "8          lot   1.51\n",
       "9         pods   1.44"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "score = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    x, y = LSA_topics(topic1.LSA[i])\n",
    "    words.append(x)\n",
    "    score.append(y)\n",
    "\n",
    "topic1_df = pd.DataFrame()\n",
    "topic1_df['words'] = words\n",
    "topic1_df['score'] = score\n",
    "\n",
    "words = []\n",
    "score = []\n",
    "\n",
    "for i in range(10,20):\n",
    "    x, y = LSA_topics(topic2.LSA[i])\n",
    "    words.append(x)\n",
    "    score.append(y)\n",
    "\n",
    "topic2_df = pd.DataFrame()\n",
    "topic2_df['words'] = words\n",
    "topic2_df['score'] = score\n",
    "\n",
    "words = []\n",
    "score = []\n",
    "\n",
    "for i in range(20,30):\n",
    "    x, y = LSA_topics(topic3.LSA[i])\n",
    "    words.append(x)\n",
    "    score.append(y)\n",
    "\n",
    "topic3_df = pd.DataFrame()\n",
    "topic3_df['words'] = words\n",
    "topic3_df['score'] = score\n",
    "\n",
    "words = []\n",
    "score = []\n",
    "\n",
    "for i in range(30,40):\n",
    "    x, y = LSA_topics(topic4.LSA[i])\n",
    "    words.append(x)\n",
    "    score.append(y)\n",
    "\n",
    "topic4_df = pd.DataFrame()\n",
    "topic4_df['words'] = words\n",
    "topic4_df['score'] = score\n",
    "topic4_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic1_df.to_csv('topic1.csv')\n",
    "topic2_df.to_csv('topic2.csv')\n",
    "topic3_df.to_csv('topic3.csv')\n",
    "topic4_df.to_csv('topic4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
